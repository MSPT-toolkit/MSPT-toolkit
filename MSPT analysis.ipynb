{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c1b241",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf214c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import trackpy\n",
    "import os\n",
    "import h5py\n",
    "import time\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import mspt.particle_fitting as fit\n",
    "import mspt.image_processing as img\n",
    "import mspt.particle_detection as detect\n",
    "import mspt.trajectory_analysis as traj\n",
    "import mspt.plotting as plot\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = '32'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de5c963",
   "metadata": {},
   "source": [
    "# `A` Files to process\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "|Input|Output|\n",
    "|:---|:---|\n",
    "|directory (str)| MP files (list)| \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b0274b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify directory\n",
    "#directory = os.path.normpath(r'C:\\Users\\admin\\Desktop\\MSPT test') # set manually\n",
    "directory = img.directoryDialog(os.getcwd()) # or via dialog\n",
    "\n",
    "# Generate list of .mp or .h5 files to process\n",
    "filepaths_data = img.find_filepaths(directory, extension='mp', exclude=None)\n",
    "print(filepaths_data)\n",
    "assert len(filepaths_data) > 0, 'Cannot find any HDF5 files to process in current directory'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ffd85c",
   "metadata": {},
   "source": [
    "# `B` Movie processing\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<h4><center> Parameters </center></h4>\n",
    "\n",
    "|Load movie|Background removal|Spot detection|Trajectory linking|\n",
    "|:---|:---|:---|:---|\n",
    "|`batch_mode` (bool)  | `mode` (str)                  | `thresh` (float)     | `dmax` (float)| \n",
    "|`frame_range` (list) | `window_length` (int)         |`DoG_estimates` (dict)|`max_frames_to_vanish` (int)| \n",
    "|`navg` (int)         | `save_processed_movies` (bool)|                      | `minimum_trajectory_length` (int)| \n",
    "|                     | `parallel` (bool)             |                      | |\n",
    "|                     |`GPU` (bool)                   |                      | |\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "|Input|Output|\n",
    "|:---|:---|\n",
    "|MP files (list)| background-processed movie (HDF5 file)| \n",
    "|               | detection results (CSV file)| \n",
    "|               | linked trajectories (CSV file)| \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80b9464",
   "metadata": {},
   "source": [
    "## `B.1` Parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9036d66a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# General parameters\n",
    "batch_mode = True # Load mp file(s) on the fly without pop-up file dialog\n",
    "frame_range = [] # Restrict analysis to certain frames, e.g. [0, 2000]. To analyze whole movie, leave list empty.\n",
    "navg = 1 # Frame averaging, applied before background removal\n",
    "assert len(frame_range)==2 or len(frame_range)==0, 'frame_range is expected to be either of type [] or [int, int]'\n",
    "\n",
    "# Background removal\n",
    "mode = 'continuous_median' # Choose background removal strategy\n",
    "window_length = 101 # Set median window length\n",
    "save_processed_movies = True # Save movies after background removal\n",
    "parallel = True # Use multiple cores to perform background substraction\n",
    "GPU = False # Use GPU to perform background substraction (requires CUDA and pytorch)\n",
    "\n",
    "# Spot detection\n",
    "thresh = 0.00055 # Threshold paramter to mask candidate spots\n",
    "DoG_estimates={ 'T' : 0.1423, 's' : 2.1436, 'sigma' : 1.2921 } # Initial guesses for PSF fitting\n",
    "\n",
    "# Trajectory linking parameters:\n",
    "dmax = 4. # Maximum displacement of particles per frame (in pixels)\n",
    "max_frames_to_vanish = 0 # Allow to link particles that where missed in these many frames\n",
    "minimum_trajectory_length = 5 # Only keep particle that exist for at least this many frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce32085",
   "metadata": {},
   "source": [
    "## `B.2` Background removal, spot detection, and linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9a552d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for filename in filepaths_data:\n",
    "    \n",
    "    # Apply continuous median background removal\n",
    "    frames, file = img.mp_reader(batch_mode=batch_mode,\n",
    "                                 file_to_load=filename,\n",
    "                                 frame_range=frame_range,\n",
    "                                 mode=mode,\n",
    "                                 navg=navg,\n",
    "                                 window_length=window_length,\n",
    "                                 parallel=parallel, \n",
    "                                 GPU=GPU)\n",
    "    \n",
    "    # Get name of video\n",
    "    name = os.path.splitext(os.path.basename(file))[0]\n",
    "    # Get current timestamp to prevent overwrite data\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Save processed movies to HDF5 file\n",
    "    if save_processed_movies:\n",
    "        saving_folder_movie = os.path.splitext(file)[0] + '_{}{}'.format(mode[11:], window_length)\n",
    "        processed_movie_file = os.path.join(saving_folder_movie,\n",
    "                                            os.path.split(saving_folder_movie)[1] + '.h5')\n",
    "        \n",
    "        if not os.path.exists(saving_folder_movie):\n",
    "            os.makedirs(saving_folder_movie)\n",
    "        \n",
    "        with h5py.File(processed_movie_file, 'w') as fn:\n",
    "            fn.create_dataset('frames', data=frames)\n",
    "            fn.create_dataset('window_length', data=window_length)\n",
    "            if frame_range:\n",
    "                fn.create_dataset('frame_range', data=frame_range)\n",
    "            else:\n",
    "                fn.create_dataset('frame_range', data=[0, frames.shape[0]])\n",
    "        print('Saved processed movies to {}'.format(processed_movie_file))\n",
    "\n",
    "        \n",
    "    # Detect and fit candidate spots\n",
    "    fitted_particles = fit.particle_fitter_parallel(frames,\n",
    "                                                    halfsize=window_length//2,\n",
    "                                                    thresh=thresh,\n",
    "                                                    frame_range=[],\n",
    "                                                    method='trust-ncg',\n",
    "                                                    DoG_estimates=DoG_estimates)     \n",
    "    \n",
    "    # Create folder to save processed data\n",
    "    if save_processed_movies:\n",
    "        detections_folder = os.path.join(saving_folder_movie,\n",
    "                                         'thresh{}_fits{}'.format(thresh, timestamp))\n",
    "    else:\n",
    "        detections_folder = '{}_{}{}_thresh{}_fits{}'.format(os.path.splitext(file)[0], # Remove file extension\n",
    "                                                             mode[11:], # Remove 'continuous_'\n",
    "                                                             window_length,\n",
    "                                                             thresh,\n",
    "                                                             timestamp)   \n",
    "        \n",
    "    if not os.path.exists(detections_folder):\n",
    "        os.makedirs(detections_folder)    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    # Save particle detections as csv file\n",
    "    if len(frame_range) == 0:\n",
    "        csv_name = '{}_all_frames'.format(name)\n",
    "        detections_file = os.path.join(detections_folder,\n",
    "                                       csv_name + '.csv')  \n",
    "    else:\n",
    "        csv_name = '{}_frames{}-{}'.format(name, frame_range[0], frame_range[1])\n",
    "        detections_file = os.path.join(detections_folder,\n",
    "                                       csv_name + '.csv')\n",
    "    \n",
    "    fitted_particles.to_csv(detections_file)\n",
    "    print('Saved trajectory list to {}'.format(detections_file))\n",
    "\n",
    "    \n",
    "    # Link trajectories\n",
    "    linked_trajectories = trackpy.link_df(fitted_particles, search_range=dmax, memory=max_frames_to_vanish)\n",
    "    linked_trajectories = linked_trajectories.sort_values(by=['particle', 'frame'])\n",
    "    trajectories_lenfilt = trackpy.filter_stubs(linked_trajectories, minimum_trajectory_length)\n",
    "\n",
    "    trajectories_folder = os.path.join(detections_folder,\n",
    "                                       'dmax{}_mem{}_fits{}'.format(dmax, max_frames_to_vanish, timestamp))\n",
    "    \n",
    "    if not os.path.exists(trajectories_folder):\n",
    "        os.makedirs(trajectories_folder)\n",
    "\n",
    "    trajectories_file = os.path.join(trajectories_folder,\n",
    "                                     '{}_trajectories.csv'.format(csv_name))\n",
    "\n",
    "#     # Workaround in Windows if filename is too long\n",
    "#     if len(trajectories_file) >= 260: #max is 260\n",
    "#         trajectories_file = '\\\\\\\\?\\\\'+ trajectories_file\n",
    "\n",
    "    trajectories_lenfilt.to_csv(trajectories_file)\n",
    "    print('Saved trajectory list to {}'.format(trajectories_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ef6576",
   "metadata": {},
   "source": [
    "## `B.3` Alternatively, load already processed movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a6f6e4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h4><center> Parameters </center></h4>\n",
    "\n",
    "|Spot detection|Trajectory linking|\n",
    "|:---|:---|\n",
    "| `thresh` (float)     | `dmax` (float)| \n",
    "|`DoG_estimates` (dict)|`max_frames_to_vanish` (int)| \n",
    "|                      | `minimum_trajectory_length` (int)| \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "|Input|Output|\n",
    "|:---|:---|\n",
    "|HDF5 files (list)| detection results (CSV file)| \n",
    "|| linked trajectories (CSV file)| \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce5d6d9",
   "metadata": {},
   "source": [
    "### `B.3.1` Pre-processed movies to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0048c1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify directory containing background-corrected movies\n",
    "#directory = os.path.normpath(r'C:\\Users\\admin\\Desktop\\MSPT example') # set manually\n",
    "directory = img.directoryDialog(os.getcwd()) # or via dialog\n",
    "\n",
    "# Generate list of .h5 files to process\n",
    "filepaths_data = img.find_filepaths(directory, extension='h5', exclude=None)\n",
    "print(filepaths_data)\n",
    "assert len(filepaths_data) > 0, 'Cannot find any HDF5 files to process in current directory'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6f3b88",
   "metadata": {},
   "source": [
    "### `B.3.2` Parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12618a22",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Analyze subset of saved movie\n",
    "frame_range = [1000 ,5000] # Restrict analysis to certain frames, e.g. [0, 2000]. To analyze whole movie, leave list empty.\n",
    "assert len(frame_range)==2 or len(frame_range)==0, 'frame_range is expected to be either of type [] or [int, int]'\n",
    "\n",
    "# Spot detection\n",
    "thresh = 0.00055 # Threshold paramter to mask candidate spots\n",
    "DoG_estimates={ 'T' : 0.1423, 's' : 2.1436, 'sigma' : 1.2921 } # Initial guesses for PSF fitting\n",
    "\n",
    "# Trajectory linking parameters:\n",
    "dmax = 4. # Maximum displacement of particles per frame (in pixels)\n",
    "max_frames_to_vanish = 0 # Allow to link particles that where missed in these many frames\n",
    "minimum_trajectory_length = 5 # Only keep particle that exist for at least this many frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa94fd0c",
   "metadata": {},
   "source": [
    "### `B.3.3` Spot detection and linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95be3d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for file in filepaths_data:\n",
    "    # Load processed movies from HDF5 file\n",
    "    with h5py.File(file, 'r') as h5_file:\n",
    "        frames = np.asarray(h5_file['frames']).copy()\n",
    "        window_length = np.asarray(h5_file['window_length']).copy()\n",
    "        frame_range_h5 = np.asarray(h5_file['frame_range']).copy()\n",
    "        if frame_range:\n",
    "            assert frame_range[0] >= frame_range_h5[0], 'Requested frame range not contained in processed movie'\n",
    "            assert frame_range[1] <= frame_range_h5[1], 'Requested frame range not contained in processed movie'\n",
    "            frames = frames[ frame_range[0]-frame_range_h5[0] : frame_range[1]-frame_range_h5[0] ]\n",
    "            \n",
    "    print('Loaded processed movie {}'.format(file))\n",
    "    \n",
    "    # Get name of video\n",
    "    name = os.path.splitext(os.path.basename(file))[0]\n",
    "    # Get current timestamp to prevent overwrite data\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "        \n",
    "    # Detect and fit candidate spots\n",
    "    fitted_particles = fit.particle_fitter_parallel(frames,\n",
    "                                                    halfsize=window_length//2,\n",
    "                                                    thresh=thresh,\n",
    "                                                    method='trust-ncg',\n",
    "                                                    DoG_estimates=DoG_estimates)     \n",
    "    \n",
    "    # Create folder to save processed data\n",
    "    saving_folder = os.path.dirname(file)\n",
    "    detections_folder = os.path.join(saving_folder,\n",
    "                                         'thresh{}_fits{}'.format(thresh, timestamp))\n",
    "        \n",
    "    if not os.path.exists(detections_folder):\n",
    "        os.makedirs(detections_folder)    \n",
    "\n",
    "\n",
    "    # Save particle detections as csv file\n",
    "    if len(frame_range) == 0:\n",
    "        csv_name = '{}_all_frames'.format(name)\n",
    "        detections_file = os.path.join(detections_folder,\n",
    "                                       csv_name + '.csv')  \n",
    "    else:\n",
    "        csv_name = '{}_frames{}-{}'.format(name, frame_range[0], frame_range[1])\n",
    "        detections_file = os.path.join(detections_folder,\n",
    "                                       csv_name + '.csv')\n",
    "    \n",
    "    fitted_particles.to_csv(detections_file)\n",
    "    print('Saved trajectory list to {}'.format(detections_file))\n",
    "\n",
    "    \n",
    "    # Link trajectories\n",
    "    linked_trajectories = trackpy.link_df(fitted_particles, search_range=dmax, memory=max_frames_to_vanish)\n",
    "    linked_trajectories = linked_trajectories.sort_values(by=['particle', 'frame'])\n",
    "    trajectories_lenfilt = trackpy.filter_stubs(linked_trajectories, minimum_trajectory_length)\n",
    "    \n",
    "    trajectories_folder = os.path.join(detections_folder,\n",
    "                                       'dmax{}_mem{}_fits{}'.format(dmax, max_frames_to_vanish, timestamp))\n",
    "    \n",
    "    if not os.path.exists(trajectories_folder):\n",
    "        os.makedirs(trajectories_folder)\n",
    "\n",
    "    trajectories_file = os.path.join(trajectories_folder,\n",
    "                                     '{}_trajectories.csv'.format(csv_name))\n",
    "\n",
    "#     # Workaround in Windows if filename is too long\n",
    "#     if len(trajectories_file) >= 260: #max is 260\n",
    "#         trajectories_file = '\\\\\\\\?\\\\'+ trajectories_file\n",
    "\n",
    "    trajectories_lenfilt.to_csv(trajectories_file)\n",
    "    print('Saved trajectory list to {}'.format(trajectories_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da166851",
   "metadata": {},
   "source": [
    "# `C` Trajectory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c40e087",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h4><center> Parameters </center></h4>\n",
    "\n",
    "|Movie acquisition|\n",
    "|:---|\n",
    "| `frame_rate` (float)| \n",
    "| `pixel_size` (float)|\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "|Input|Output|\n",
    "|:---|:---|\n",
    "|CSV files (list)| analyzed trajectories (HDF5 file)| \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ec3dca",
   "metadata": {},
   "source": [
    "## `C.1` Parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552b1e7a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Movie acquisition parameters:\n",
    "frame_rate = 199.8 # in Hz\n",
    "pixel_size = 84.4 # in nm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8420fc26",
   "metadata": {},
   "source": [
    "## `C.2` Files to process (*.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c36b9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify directory with csv files\n",
    "#directory = os.path.normpath(r'C:\\Users\\admin\\Desktop\\MSPT results') # set manually\n",
    "directory = img.directoryDialog(os.getcwd()) # or via dialog\n",
    "assert os.path.isdir(directory), 'Directory \"{}\" does not exist'.format(directory)\n",
    "\n",
    "# Create list of csv files containing trajectories\n",
    "csv_files = traj.get_csv_files(directory)\n",
    "print(csv_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dbc154",
   "metadata": {},
   "source": [
    "## `C.3` Output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5cbb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify filename of the results file \n",
    "filename = 'MSPT result_3.h5'\n",
    "if os.path.isabs(filename):\n",
    "    results_file = filename\n",
    "else:\n",
    "    results_file = os.path.join(directory,filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0907276b",
   "metadata": {},
   "source": [
    "## `C.4` Fit mean squared displacement (MSD) and jump distance distribution (JDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b93f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit trajectories\n",
    "traj.fit_trajectories(csv_files,\n",
    "                      os.path.join(results_file),\n",
    "                      frame_rate=frame_rate,\n",
    "                      pixel_size=pixel_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65ae7a6",
   "metadata": {},
   "source": [
    "## `C.5` Contrast-to-mass conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd203f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Microscope-specific calibration parameters\n",
    "slope = 28191.37194436 # in kDa/'contrast'\n",
    "offset = -20.47852753 # in kDa\n",
    "\n",
    "# Load HDF5 file where results of the trajectory analysis are stored\n",
    "# Convert contrast to mass using the linear relationship obtained from calibration measurements\n",
    "with pd.HDFStore(results_file, 'r+') as dfs:\n",
    "    for key in dfs.keys():\n",
    "        df = dfs[key].copy()\n",
    "        #df['median_mass'] = slope * df['med_c'] + offset\n",
    "        df['median_mass'] = traj.apply_calibration(df, slope=slope, offset=offset)\n",
    "        dfs[key] = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf5ba0f",
   "metadata": {},
   "source": [
    "## `C.6` Estimate particle density on membrane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88cf682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HDF5 file where results of the trajectory analysis are stored\n",
    "# Calculate the number of particles present for each trajectory 'particle number (linked)'\n",
    "with pd.HDFStore(results_file, 'r+') as dfs:\n",
    "    for key in dfs.keys():\n",
    "        df = dfs[key].copy()\n",
    "        df['particle number (linked)'] = traj.calc_particle_number_linked(df)\n",
    "        #df['particle number (detected)'] = traj.calc_particle_number_detected(df)\n",
    "        dfs[key] = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8ef39b",
   "metadata": {},
   "source": [
    "# `D` Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab713f63",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h4><center> Plotting parameters </center></h4>\n",
    "\n",
    "|X variable|Y variable| Axis limits | Filter trajectories |Misc\n",
    "|:---|:---|:---|:---|:---|\n",
    "|`median_mass`| `Deff_global` | `x_range` | `traj_length` | `figsize`  |\n",
    "|`mean_mass`  | `Deff_JDD`    | `y_range` | `density`     | `n_levels` |\n",
    "|`med_c`      | `D_MSD_n4`    |           |               | `cmap`     |\n",
    "|`mean_c`     |               |           |               | `alpha`    |\n",
    "|             |               |           |               | `show`     |\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "|Input|Output|\n",
    "|:---|:---|\n",
    "|MSPT results (DataFrame)| 2D-KDE plot (mpl figure/PDF)| \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3ae945",
   "metadata": {},
   "source": [
    "## `D.1` Input HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bf11fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify HDF5 file containing MSPT results\n",
    "#results_file = os.path.normpath(r\"C:\\Users\\admin\\Desktop\\MSPT test\\MSPT result.h5\") # set manually\n",
    "results_file = img.fileDialog(os.getcwd()) # or via file dialog\n",
    "assert os.path.isfile(results_file), 'File \"{}\" does not exist'.format(results_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f592b097",
   "metadata": {},
   "source": [
    "## `D.2` Copy data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e965fc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data containers into memory as a dictionary\n",
    "data = dict()\n",
    "with pd.HDFStore(results_file, 'r') as dfs:\n",
    "    for key in dfs.keys():\n",
    "        data[key] = dfs[key].copy()     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9db044f",
   "metadata": {},
   "source": [
    "## `D.3` Pool data (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7e9117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pool data to get a single DataFrame\n",
    "data_pooled = pd.concat(data,axis=0)\n",
    "print(data_pooled.shape)\n",
    "print(data_pooled.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0048894e",
   "metadata": {},
   "source": [
    "## `D.4` Generate correlation plot of mass and diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e05935",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot data\n",
    "fig, axs, df_kde, dat = plot.generate_2D_KDE(data_pooled,\n",
    "                                             x='median_mass',\n",
    "                                             y='Deff_global',\n",
    "                                             x_range=(0,400), \n",
    "                                             y_range=(-1,1), # in log space\n",
    "                                             figsize=(5,5),\n",
    "                                             traj_length=5,\n",
    "                                             density=None,\n",
    "                                             n_levels=12,\n",
    "                                             cmap=mpl.cm.gray_r,\n",
    "                                             alpha=1.0,\n",
    "                                             show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5622725d",
   "metadata": {},
   "source": [
    "## `D.5` Save figure to pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf065cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save plot to pdf\n",
    "fig.savefig(r\"C:\\Users\\admin\\Desktop\\MSPT example\\MSPT_results_pooled.pdf\",transparent=True,bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
